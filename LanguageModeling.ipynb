{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/dasmiq/CS6120-HW2/blob/master/LanguageModeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "moWB9udaKesP"
   },
   "source": [
    "Name Thinh Lam\n",
    "\n",
    "\n",
    "Your task is to train *character-level* language models. \n",
    "You will train unigram, bigram, and trigram character-level models on a collection of books from Project Gutenberg. You will then use these trained English language models to distinguish English documents from Brazilian Portuguese documents in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "id": "gHFJmuftHJld",
    "outputId": "7879ca79-e483-400a-f860-4fd46fc59291"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hongt\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\hongt\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\hongt\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package mac_morpho to\n",
      "[nltk_data]     C:\\Users\\hongt\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package mac_morpho is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import httpimport\n",
    "\n",
    "with httpimport.remote_repo(['lm_helper'], 'https://raw.githubusercontent.com/jasoriya/CS6120-PS2-support/master/utils/'):\n",
    "    from lm_helper import get_train_data, get_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "import collections\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "6x0pfuiEChTh"
   },
   "outputs": [],
   "source": [
    "# get the train and test data\n",
    "train = get_train_data()\n",
    "test, test_files = get_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.6"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)*0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, validation = train_test_split(train,test_size= 0.2, random_state=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createNgramFeatures(corpus,n1=1, n2=1):\n",
    "    cv = CountVectorizer(analyzer='char',ngram_range=(n1,n2))\n",
    "    texts = cv.fit_transform([record for record in corpus]).toarray()\n",
    "    char = cv.get_feature_names()\n",
    "    return texts, char\n",
    "\n",
    "# This cells is to join words of a sentence together in each book. \n",
    "def create_corpus(train_ds):\n",
    "    books = []\n",
    "    for i in range(len(train_ds)):\n",
    "        for sent in train_ds[i]:\n",
    "            books.append(\" \".join(sent).lstrip().rstrip())\n",
    "    return books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = create_corpus(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Oh4VOoiSIoUF"
   },
   "outputs": [],
   "source": [
    "unigram, unigram_char = createNgramFeatures(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'\\x1a': 1, ' ': 1, '!': 1, '\"': 1, '$': 1, '%': 1, '&': 1, \"'\": 1, '(': 1, ')': 1, '*': 1, '+': 1, ',': 1, '-': 1, '.': 1, '/': 1, '0': 1, '1': 1, '2': 1, '3': 1, '4': 1, '5': 1, '6': 1, '7': 1, '8': 1, '9': 1, ':': 1, ';': 1, '<': 1, '=': 1, '>': 1, '?': 1, '@': 1, '[': 1, ']': 1, '_': 1, '`': 1, 'a': 1, 'b': 1, 'c': 1, 'd': 1, 'e': 1, 'f': 1, 'g': 1, 'h': 1, 'i': 1, 'j': 1, 'k': 1, 'l': 1, 'm': 1, 'n': 1, 'o': 1, 'p': 1, 'q': 1, 'r': 1, 's': 1, 't': 1, 'u': 1, 'v': 1, 'w': 1, 'x': 1, 'y': 1, 'z': 1, '}': 1, '~': 1, 'æ': 1, 'è': 1, 'é': 1, 'î': 1})\n"
     ]
    }
   ],
   "source": [
    "#Check if there are any weird characters in the corpus.\n",
    "unigram_vocab_count = collections.Counter(unigram_char)\n",
    "print(unigram_vocab_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace those weird characters with \"^\" which is not in the corpus.\n",
    "corpus_char = []\n",
    "for item in corpus:\n",
    "    corpus_char.append(re.sub('[/$/&/%/>/</~/}/{/*/@/`/æ/é/è/î/=/+/\\x1a]',\"^\" ,item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram1, unigram_char = createNgramFeatures(corpus_char)\n",
    "bigram1, bigram_char = createNgramFeatures(corpus_char,2,2)\n",
    "trigram1, trigram_char = createNgramFeatures(corpus_char,3,3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unigram rows: 98552, unigram columns: 52\n",
      "bigram rows: 98552, bigram columns: 936\n",
      "trigram rows: 98552, trigram columns: 9099\n"
     ]
    }
   ],
   "source": [
    "print(\"unigram rows: {}, unigram columns: {}\".format(unigram1.shape[0], unigram1.shape[1]))\n",
    "print(\"bigram rows: {}, bigram columns: {}\".format(bigram1.shape[0], bigram1.shape[1]))\n",
    "print(\"trigram rows: {}, trigram columns: {}\".format(trigram1.shape[0], trigram1.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({' ': 1, '!': 1, '\"': 1, \"'\": 1, '(': 1, ')': 1, ',': 1, '-': 1, '.': 1, '0': 1, '1': 1, '2': 1, '3': 1, '4': 1, '5': 1, '6': 1, '7': 1, '8': 1, '9': 1, ':': 1, ';': 1, '?': 1, '[': 1, ']': 1, '^': 1, '_': 1, 'a': 1, 'b': 1, 'c': 1, 'd': 1, 'e': 1, 'f': 1, 'g': 1, 'h': 1, 'i': 1, 'j': 1, 'k': 1, 'l': 1, 'm': 1, 'n': 1, 'o': 1, 'p': 1, 'q': 1, 'r': 1, 's': 1, 't': 1, 'u': 1, 'v': 1, 'w': 1, 'x': 1, 'y': 1, 'z': 1})\n"
     ]
    }
   ],
   "source": [
    "#Check again to ensure those weird characters have been replaced. \n",
    "unigram_vocab_count = collections.Counter(unigram_char)\n",
    "print(unigram_vocab_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_count(corpus, n1=1,n2=1):\n",
    "    \n",
    "    cv = CountVectorizer(analyzer='char', ngram_range = (n1,n2))\n",
    "    ngram = cv.fit_transform(corpus)      #create a matrix of chars\n",
    "    \n",
    "    #list of characters.\n",
    "    chars = cv.get_feature_names() \n",
    "    \n",
    "    #add up values all rows in each columns. The first row represent the count of each char.\n",
    "    count_char = np.array(np.sum(ngram,axis=0))[0]\n",
    "    \n",
    "    #create a dictionary of each character and its count value. \n",
    "    ngram = dict(zip(chars, count_char))\n",
    "    \n",
    "    return ngram, sum(count_char)\n",
    "    \n",
    "    \n",
    "uni, TOTAL_UNI = ngram_count(corpus_char) \n",
    "bi, TOTAL_BI = ngram_count(corpus_char,2,2) \n",
    "tri, TOTAL_TRI = ngram_count(corpus_char,3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12010505\n",
      "11911953\n",
      "11813477\n"
     ]
    }
   ],
   "source": [
    "#print(uni)\n",
    "#print(bi)\n",
    "#print(tri)\n",
    "\n",
    "#TOTAL_UNI = sum(uni.values())\n",
    "#TOTAL_BI = sum(bi.values())\n",
    "#TOTAL_TRI = sum(tri.values())\n",
    "\n",
    "print(TOTAL_UNI)\n",
    "print(TOTAL_BI)\n",
    "print(TOTAL_TRI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "def unigram_probability(uni_char):\n",
    "    if uni_char in uni.keys():\n",
    "        return uni[uni_char]/TOTAL_UNI \n",
    "    return uni['^']/TOTAL_UNI\n",
    "    \n",
    "def bigram_probability(bi_char):\n",
    "    if bi_char in bi.keys():\n",
    "        return bi[bi_char]/uni[bi_char[0]]\n",
    "    return 0\n",
    "\n",
    "def trigram_probability(tri_char):\n",
    "    if tri_char in tri.keys():\n",
    "        return tri[tri_char]/bi[tri_char[:2]]\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_interpolation_perplexity(train_ds,L1,L2,L3):\n",
    "    results=[]\n",
    "    \n",
    "    def create_trigram(sentence):\n",
    "        return [sentence[i:i+3] for i in range(len(sentence)-3)]\n",
    "    \n",
    "    for book in train_ds:\n",
    "        cv = CountVectorizer(analyzer='char')\n",
    "        matrix = cv.fit_transform(book)\n",
    "        total_char = sum(np.array(np.sum(matrix,axis=0))[0])\n",
    "        \n",
    "        sum_log_prob = 0\n",
    "        for sentence in book: \n",
    "            for tri_gram in create_trigram(sentence):\n",
    "                probability = L1 * unigram_probability(tri_gram[-1]) + L2 * bigram_probability(tri_gram[0:2]) + L3 * trigram_probability(tri_gram)\n",
    "                sum_log_prob += math.log(probability,2)\n",
    "\n",
    "        \n",
    "        perplexity = 2**(-sum_log_prob/total_char)\n",
    "        results.append(perplexity)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAMBDA_SETS =[[0.11,0.09,0.8],\n",
    "             [0.05,0.05,0.9],\n",
    "             [0.9,0.05,0.05],\n",
    "             [0.17,0.13,0.7],\n",
    "             [0.25,0.6,0.15],\n",
    "             [0.23,0.17,0.6],\n",
    "             [0.13,0.27,0.6],\n",
    "             [0.1,0.18,0.72],\n",
    "             [0.1,0.1,0.8]]\n",
    "\n",
    "\n",
    "def create_test(testset):\n",
    "    results=[]\n",
    "    for i in range(len(testset)):\n",
    "        sentences=[]\n",
    "        for j in range(len(testset[i])):\n",
    "            sentences.append((\" \".join(testset[i][j]).lstrip().rstrip()))\n",
    "        results.append(sentences)\n",
    "    return results\n",
    "\n",
    "vals = create_test(validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1: 0.11, L2: 0.09, L3: 0.8, pp1: 7.238311163593336, pp2: 6.656228381379482 , pp3: 7.877630967954666, pp4: 7.92132629371633\n",
      "L1: 0.05, L2: 0.05, L3: 0.9, pp1: 7.646427035120186, pp2: 6.918109643452387 , pp3: 8.474975729008921, pp4: 8.742446669413381\n",
      "L1: 0.9, L2: 0.05, L3: 0.05, pp1: 13.450811257722803, pp2: 13.25106522481023 , pp3: 13.94735015408296, pp4: 12.228459658919187\n",
      "L1: 0.17, L2: 0.13, L3: 0.7, pp1: 7.0965398569076905, pp2: 6.598234462964519 , pp3: 7.643070590060275, pp4: 7.569056254016475\n",
      "L1: 0.25, L2: 0.6, L3: 0.15, pp1: 7.973727202855698, pp2: 7.707689734584803 , pp3: 8.428911739065006, pp4: 8.15049942887561\n",
      "L1: 0.23, L2: 0.17, L3: 0.6, pp1: 7.0951963890833785, pp2: 6.656348371609629 , pp3: 7.584790254769845, pp4: 7.423166127185495\n",
      "L1: 0.13, L2: 0.27, L3: 0.6, pp1: 6.734976816695541, pp2: 6.293488937941123 , pp3: 7.2459577838664835, pp4: 7.265958670019745\n",
      "L1: 0.1, L2: 0.18, L3: 0.72, pp1: 6.858886740523797, pp2: 6.3481379212831905 , pp3: 7.434660254103765, pp4: 7.516019005906045\n",
      "L1: 0.1, L2: 0.1, L3: 0.8, pp1: 7.177321058155648, pp2: 6.597962660028258 , pp3: 7.816792576140555, pp4: 7.8881461798600965\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(LAMBDA_SETS)):\n",
    "    pp= linear_interpolation_perplexity(vals,LAMBDA_SETS[i][0],LAMBDA_SETS[i][1],LAMBDA_SETS[i][2])\n",
    "    print(\"L1: {}, L2: {}, L3: {}, pp1: {}, pp2: {} , pp3: {}, pp4: {}\".format(LAMBDA_SETS[i][0],\n",
    "                                                                               LAMBDA_SETS[i][1],\n",
    "                                                                               LAMBDA_SETS[i][2],\n",
    "                                                                               pp[0],pp[1],pp[2],pp[3]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My best Lambda_set is 0.13,0.27,0.6 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RS3mnaIvQnhI"
   },
   "source": [
    "## 1.2\n",
    "Calculate the perplexity for each document in the test set using the linear interpolation smoothing method. For determining λs for linear interpolation of the trigram, bigram, and unigram models, you can divide the training data into a new training set (80%) and a held-out set (20%).\n",
    "Then choose ~10 random pairs of $(\\lambda_3, \\lambda_2)$ such that $1 > \\lambda_3 > \\lambda_2 > 0$ and $\\sum_{i=1}^3 \\lambda_i = 1$ to test on held-out data.\n",
    "\n",
    "Some documents in the test set are in Brazilian Portuguese. Identify them as follows: \n",
    "  - Sort by perplexity and set a cut-off threshold. All the documents above this threshold score should be categorized as Brazilian Portuguese. \n",
    "  - Print the file names (from `test_files`) and perplexities of the documents above the threshold\n",
    "\n",
    "    ```\n",
    "        file name, score\n",
    "        file name, score\n",
    "        . . .\n",
    "        file name, score\n",
    "    ```\n",
    "\n",
    "  - Copy this list of filenames and manually annotate them as being correctly or incorrectly labeled as Portuguese.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "QQF4HhQGOZD8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.990035755187394\n",
      "9.183538995430567\n",
      "8.494066898830214\n",
      "7.423244135292134\n",
      "7.724299802362842\n",
      "8.093941902921818\n",
      "16.113449087468254\n",
      "15.422144980722324\n",
      "6.669571270843347\n",
      "9.9386005059876\n",
      "6.793393943861492\n",
      "7.289269848050166\n",
      "7.523691034925563\n",
      "7.710589420243429\n",
      "6.787827635854505\n",
      "8.86372475666537\n",
      "6.849153530803214\n",
      "16.609484975479575\n",
      "7.310579881198023\n",
      "7.492478421056932\n",
      "6.494627472042587\n",
      "8.814281809223031\n",
      "6.609634813179062\n",
      "7.9027001691176615\n",
      "6.6808534891452\n",
      "7.936628955413524\n",
      "7.67751963755789\n",
      "7.984615697583378\n",
      "7.595579033825983\n",
      "8.725589922615477\n",
      "7.595843932592441\n",
      "7.89268370317403\n",
      "8.437217867895308\n",
      "7.4925201334099905\n",
      "8.262544667233222\n",
      "8.12252936871801\n",
      "6.576473278255494\n",
      "7.935411173344955\n",
      "6.40457886825349\n",
      "8.155366269749901\n",
      "8.188886650904761\n",
      "16.137268031378238\n",
      "7.765881237969996\n",
      "15.727599422118397\n",
      "7.550451395248099\n",
      "7.870753624166888\n",
      "6.821253564422213\n",
      "8.315113720167242\n",
      "9.13132145147403\n",
      "7.50652053014766\n",
      "8.06679995598811\n",
      "7.138020621336395\n",
      "7.507867530134214\n",
      "7.789421001773431\n",
      "9.314093916774384\n",
      "6.562499038078807\n",
      "7.981090196230391\n",
      "7.8430492581247515\n",
      "8.39032453808837\n",
      "14.664377618553951\n",
      "8.286248362935533\n",
      "7.473806282503056\n",
      "10.599886162106388\n",
      "8.143353393033788\n",
      "7.7047479015869405\n",
      "8.316967351613732\n",
      "8.32184888635106\n",
      "7.273513611428669\n",
      "7.24087937522585\n",
      "6.499863683466397\n",
      "8.982418559367094\n",
      "7.396366822647291\n",
      "9.093847586057358\n",
      "7.873371655294519\n",
      "7.689419984430688\n",
      "6.817067665697121\n",
      "7.6238335910304516\n",
      "8.316183126537819\n",
      "9.744572241055916\n",
      "7.441809187713508\n",
      "8.231760959641983\n",
      "7.805577107329496\n",
      "16.049874235796754\n",
      "8.063872335665248\n",
      "8.543343271821662\n",
      "7.429085420641374\n",
      "7.574920335273866\n",
      "15.98207971341842\n",
      "6.758691591274295\n",
      "8.481471919219675\n",
      "7.554731536672346\n",
      "7.981058759137541\n",
      "8.331092180994348\n",
      "8.461091566914016\n",
      "6.858360770534081\n",
      "7.2905840606139245\n",
      "6.568715262065757\n",
      "7.3595643134481\n",
      "15.763092469916936\n",
      "7.52764454188602\n",
      "15.956685363245704\n",
      "8.70105156378916\n",
      "8.518848691232408\n",
      "7.84126668134512\n",
      "8.100904393518316\n",
      "7.611600282499064\n",
      "6.764591251456569\n",
      "8.934643871969996\n",
      "8.61570674829581\n",
      "7.912720774713701\n",
      "7.716610197817065\n",
      "7.601914169119568\n",
      "6.924028081658149\n",
      "7.818648720735263\n",
      "8.250057311249423\n",
      "7.809461174362079\n",
      "8.669419538340735\n",
      "7.413925599319639\n",
      "7.9764966999742155\n",
      "7.589717658762484\n",
      "7.848922053549117\n",
      "7.991322916699226\n",
      "15.616903162553047\n",
      "16.146895206155964\n",
      "6.633977229402053\n",
      "6.947565412258418\n",
      "7.495525936592012\n",
      "7.192669034189254\n",
      "8.35691620483602\n",
      "7.897151455739346\n",
      "8.205744532901043\n",
      "9.032542359902482\n",
      "15.894472706141903\n",
      "15.827872224904002\n",
      "7.06337411970633\n",
      "7.86204014062137\n",
      "6.65285255376096\n",
      "8.496880798484344\n",
      "6.714578879293527\n",
      "6.538028734757188\n",
      "10.006179237446318\n",
      "6.947679095960854\n",
      "7.352927289682936\n",
      "8.214630290456256\n",
      "7.039487331473565\n",
      "7.061066776756135\n",
      "7.169852525182931\n",
      "8.408897535880044\n",
      "8.42988417000296\n",
      "7.5515001378790325\n",
      "7.289377125038798\n",
      "7.478889917569639\n",
      "6.543944724786684\n",
      "7.10162232404932\n",
      "7.229610082062848\n",
      "8.557147659156064\n",
      "8.321192491437987\n",
      "6.2837439131266235\n",
      "7.431165740780329\n",
      "7.802186343064428\n",
      "7.157472005106061\n",
      "7.690677562081615\n",
      "7.186998328199812\n",
      "16.758850852144956\n",
      "16.084414850180014\n",
      "15.64185248389626\n",
      "9.13304108881518\n",
      "11.192437979049085\n",
      "8.497741379477503\n",
      "7.897606629287043\n",
      "16.540491995379252\n",
      "7.732258711645789\n",
      "7.53773195091733\n",
      "7.372648543325836\n",
      "7.172416128295599\n",
      "6.786056601596775\n",
      "7.419858658186077\n",
      "8.355694754170143\n",
      "8.974790243491217\n",
      "7.6147114822773005\n",
      "8.951369098400997\n",
      "7.13420408522141\n",
      "8.394711269889987\n",
      "15.48453985597181\n",
      "7.110210988986672\n",
      "6.62050943388338\n",
      "8.03713906771185\n",
      "7.1752452443191626\n",
      "7.041000303477632\n",
      "6.9982953093888955\n",
      "7.619159508114243\n",
      "9.607975128044616\n",
      "8.590845850288307\n",
      "7.4868470758650965\n",
      "7.883811394916991\n",
      "7.707049692634905\n",
      "8.823805885270287\n",
      "7.3188341676752575\n",
      "7.127388546179669\n",
      "7.726921657299867\n",
      "7.377653635625907\n",
      "7.644453758788605\n",
      "8.666015787326762\n",
      "7.590993504559196\n",
      "7.478718442878351\n",
      "7.15239558059131\n",
      "7.493980566906411\n",
      "8.304615354898539\n",
      "8.542763999644878\n",
      "6.89518705091727\n",
      "7.579320412475609\n",
      "7.308385924300494\n",
      "7.1082268145271\n",
      "8.255007544368366\n",
      "7.934358566627719\n",
      "16.030653510073087\n",
      "7.118172057730834\n",
      "8.748947740771268\n",
      "8.720379006902219\n",
      "7.507690676489884\n"
     ]
    }
   ],
   "source": [
    "test_corpus = create_test(test)\n",
    "\n",
    "#compute perplexities for the test documents to find the threshold. \n",
    "test_results=linear_interpolation_perplexity(test_corpus,0.13,0.27,0.6)\n",
    "for item in test_results:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the list of Portugese books\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>score</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>ag94fe1.txt</td>\n",
       "      <td>14.664378</td>\n",
       "      <td>Portugese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ag94ja11.txt</td>\n",
       "      <td>15.422145</td>\n",
       "      <td>Portugese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>ag94mr1.txt</td>\n",
       "      <td>15.484540</td>\n",
       "      <td>Portugese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>br94ju01.txt</td>\n",
       "      <td>15.616903</td>\n",
       "      <td>Portugese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>br94ab02.txt</td>\n",
       "      <td>15.641852</td>\n",
       "      <td>Portugese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>ag94jl12.txt</td>\n",
       "      <td>15.727599</td>\n",
       "      <td>Portugese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>ag94ab12.txt</td>\n",
       "      <td>15.763092</td>\n",
       "      <td>Portugese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>br94jl01.txt</td>\n",
       "      <td>15.827872</td>\n",
       "      <td>Portugese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>ag94no01.txt</td>\n",
       "      <td>15.894473</td>\n",
       "      <td>Portugese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>br94de01.txt</td>\n",
       "      <td>15.956685</td>\n",
       "      <td>Portugese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>ag94de06.txt</td>\n",
       "      <td>15.982080</td>\n",
       "      <td>Portugese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>br94ja04.txt</td>\n",
       "      <td>16.030654</td>\n",
       "      <td>Portugese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>ag94ju07.txt</td>\n",
       "      <td>16.049874</td>\n",
       "      <td>Portugese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>ag94ag02.txt</td>\n",
       "      <td>16.084415</td>\n",
       "      <td>Portugese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ag94ma03.txt</td>\n",
       "      <td>16.113449</td>\n",
       "      <td>Portugese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>ag94ou04.txt</td>\n",
       "      <td>16.137268</td>\n",
       "      <td>Portugese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>br94ag01.txt</td>\n",
       "      <td>16.146895</td>\n",
       "      <td>Portugese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>br94fe1.txt</td>\n",
       "      <td>16.540492</td>\n",
       "      <td>Portugese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>br94ma01.txt</td>\n",
       "      <td>16.609485</td>\n",
       "      <td>Portugese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>ag94se06.txt</td>\n",
       "      <td>16.758851</td>\n",
       "      <td>Portugese</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         filename      score   language\n",
       "59    ag94fe1.txt  14.664378  Portugese\n",
       "7    ag94ja11.txt  15.422145  Portugese\n",
       "183   ag94mr1.txt  15.484540  Portugese\n",
       "122  br94ju01.txt  15.616903  Portugese\n",
       "165  br94ab02.txt  15.641852  Portugese\n",
       "43   ag94jl12.txt  15.727599  Portugese\n",
       "98   ag94ab12.txt  15.763092  Portugese\n",
       "133  br94jl01.txt  15.827872  Portugese\n",
       "132  ag94no01.txt  15.894473  Portugese\n",
       "100  br94de01.txt  15.956685  Portugese\n",
       "87   ag94de06.txt  15.982080  Portugese\n",
       "215  br94ja04.txt  16.030654  Portugese\n",
       "82   ag94ju07.txt  16.049874  Portugese\n",
       "164  ag94ag02.txt  16.084415  Portugese\n",
       "6    ag94ma03.txt  16.113449  Portugese\n",
       "41   ag94ou04.txt  16.137268  Portugese\n",
       "123  br94ag01.txt  16.146895  Portugese\n",
       "170   br94fe1.txt  16.540492  Portugese\n",
       "17   br94ma01.txt  16.609485  Portugese\n",
       "163  ag94se06.txt  16.758851  Portugese"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "THRESHOLD = 12\n",
    "book_list=pd.DataFrame()\n",
    "book_list[\"filename\"]=test_files\n",
    "book_list[\"score\"]= test_results\n",
    "book_list=book_list.sort_values(by='score',axis=0)\n",
    "book_list[\"language\"]=book_list[\"score\"].apply(lambda x: \"English \" if x<THRESHOLD else \"Portugese\")\n",
    "\n",
    "\n",
    "print(\"the list of Portugese books\")\n",
    "display(book_list[book_list.score > THRESHOLD])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>score</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>cg04</td>\n",
       "      <td>6.283744</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>cf27</td>\n",
       "      <td>6.404579</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>cf26</td>\n",
       "      <td>6.494627</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>cf13</td>\n",
       "      <td>6.499864</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>cf36</td>\n",
       "      <td>6.538029</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>ca31</td>\n",
       "      <td>9.744572</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ca16</td>\n",
       "      <td>9.938601</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>ca17</td>\n",
       "      <td>10.006179</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>ca18</td>\n",
       "      <td>10.599886</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>ce09</td>\n",
       "      <td>11.192438</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    filename      score  language\n",
       "157     cg04   6.283744  English \n",
       "38      cf27   6.404579  English \n",
       "20      cf26   6.494627  English \n",
       "69      cf13   6.499864  English \n",
       "139     cf36   6.538029  English \n",
       "..       ...        ...       ...\n",
       "78      ca31   9.744572  English \n",
       "9       ca16   9.938601  English \n",
       "140     ca17  10.006179  English \n",
       "62      ca18  10.599886  English \n",
       "167     ce09  11.192438  English \n",
       "\n",
       "[200 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(book_list[book_list.score < THRESHOLD])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aQl2u_giVW5e"
   },
   "source": [
    "## 1.3\n",
    "Build a trigram language model with add-λ smoothing (use λ = 0.1).\n",
    "\n",
    "Sort the test documents by perplexity and perform a check for Brazilian Portuguese documents as above:\n",
    "\n",
    "  - Observe the perplexity scores and set a cut-off threshold. All the documents above this threshold score should be categorized as Brazilian Portuguese. \n",
    "  - Print the file names and perplexities of the documents above the threshold\n",
    "\n",
    "  ```\n",
    "      file name, score\n",
    "      file name, score\n",
    "      . . .\n",
    "      file name, score\n",
    "  ```\n",
    "\n",
    "  - Copy this list of filenames and manually annotate them for correctness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "IGUTEk8QUehL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Potugese files\n",
      "         filename      score   language\n",
      "59    ag94fe1.txt  28.417827  Portugese\n",
      "165  br94ab02.txt  30.289170  Portugese\n",
      "133  br94jl01.txt  30.634832  Portugese\n",
      "122  br94ju01.txt  30.672988  Portugese\n",
      "7    ag94ja11.txt  30.840945  Portugese\n",
      "100  br94de01.txt  30.968355  Portugese\n",
      "215  br94ja04.txt  31.020278  Portugese\n",
      "98   ag94ab12.txt  31.040430  Portugese\n",
      "132  ag94no01.txt  31.153239  Portugese\n",
      "170   br94fe1.txt  31.348353  Portugese\n",
      "41   ag94ou04.txt  31.573793  Portugese\n",
      "183   ag94mr1.txt  31.606144  Portugese\n",
      "43   ag94jl12.txt  31.661051  Portugese\n",
      "123  br94ag01.txt  31.760117  Portugese\n",
      "82   ag94ju07.txt  31.835033  Portugese\n",
      "6    ag94ma03.txt  31.950743  Portugese\n",
      "164  ag94ag02.txt  32.246327  Portugese\n",
      "87   ag94de06.txt  32.423947  Portugese\n",
      "163  ag94se06.txt  32.799927  Portugese\n",
      "17   br94ma01.txt  33.250555  Portugese\n"
     ]
    }
   ],
   "source": [
    "ADD_LAMBDA = 0.1\n",
    "NEW_THRESHOLD = 20\n",
    "\n",
    "def add_lambda_smoothing(trichar,lamda):\n",
    "    if trichar not in tri.keys():\n",
    "        num=lamda                    #compute numberator as 0.1 if not in trigrams\n",
    "    else:\n",
    "        num=lamda + tri[trichar]     \n",
    "    if trichar[:2] not in bi.keys():\n",
    "        den=lamda*len(uni) \n",
    "    else:\n",
    "        den= lamda*len(uni) + bi[trichar[:2]]\n",
    "    return num/den\n",
    "\n",
    "def add_smoothing_perplexity(train_ds):\n",
    "    results=[]\n",
    "    \n",
    "    def create_trigram(sentence):\n",
    "        return [sentence[i:i+3] for i in range(len(sentence)-3)]\n",
    "    \n",
    "    for book in train_ds:\n",
    "        cv = CountVectorizer(analyzer='char')\n",
    "        matrix = cv.fit_transform(book)\n",
    "        total_char = sum(np.array(np.sum(matrix,axis=0))[0])\n",
    "        \n",
    "        sum_log_prob = 0\n",
    "        for sentence in book: \n",
    "            for tri_gram in create_trigram(sentence):\n",
    "                probability = add_lambda_smoothing(tri_gram,ADD_LAMBDA)\n",
    "                sum_log_prob += math.log(probability,2)\n",
    "\n",
    "        \n",
    "        perplexity = 2**(-sum_log_prob/total_char)\n",
    "        results.append(perplexity)\n",
    "    return results\n",
    "\n",
    "test_result_2=add_smoothing_perplexity(test_corpus)\n",
    "\n",
    "df=pd.DataFrame()\n",
    "df[\"filename\"]=test_files\n",
    "df[\"score\"]=test_result_2\n",
    "df=df.sort_values(by='score',axis=0)\n",
    "\n",
    "df[\"language\"]=df[\"score\"].apply(lambda x: \"English \" if x< NEW_THRESHOLD else \"Portugese\")\n",
    "print('the list of Potugese files')\n",
    "display(df[df[\"score\"]>NEW_THRESHOLD])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    filename      score  language\n",
      "157     cg04   7.290719  English \n",
      "69      cf13   7.439201  English \n",
      "20      cf26   7.484306  English \n",
      "124     ce21   7.573957  English \n",
      "38      cf27   7.592564  English \n",
      "..       ...        ...       ...\n",
      "78      ca31  15.581845  English \n",
      "9       ca16  16.995866  English \n",
      "140     ca17  17.723653  English \n",
      "167     ce09  19.104821  English \n",
      "62      ca18  19.543667  English \n",
      "\n",
      "[200 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "display(df[df[\"score\"]<NEW_THRESHOLD])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bqhXTB5TXR25"
   },
   "source": [
    "## 1.4\n",
    "Based on your observation from above questions, compare linear interpolation and add-λ smoothing by listing out their pros and cons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tFq1ECgDI6QG"
   },
   "source": [
    "Add-lambda smoothing is better in computing the perplexities which shows the apparent threshold between English and Portugese in the test_files and it runs seemingly faster than linear interpolation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "LanguageModeling.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
